{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "logistic_regression_key.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "wTJsCDqM7Xc6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Logistic regression (you do not have to do anything in this cell)\n",
        "\n",
        "# import the libraries that we will need\n",
        "from sklearn import datasets\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# Now we load the breast cancer dataset\n",
        "X, y = datasets.load_breast_cancer(return_X_y=True)\n",
        "\n",
        "# we will only use the first feature (you should try with others after \n",
        "# completing the notebook)\n",
        "X = X[:,0]\n",
        "\n",
        "print('The shape of X is: ', X.shape)\n",
        "print('The shape of y is: ', y.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZExLCWA7iCt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# now we split the data (you do not have to do anything in this cell)\n",
        "\n",
        "# because we want to use cross validation, \n",
        "# we randomly select 10% as test, 10% as validation, and 80% as training\n",
        "Ntotal = X.shape[0]\n",
        "Ntest_val = int(Ntotal/5)\n",
        "Nval = Ntest_val/2\n",
        "Ntrain = Ntotal - Ntest_val\n",
        "\n",
        "# now let's generate the indices for the test and val\n",
        "test_val_idx = np.random.choice(range(Ntotal),Ntest_val,replace=False)\n",
        "test_idx = test_val_idx[:int(Ntest_val/2)]\n",
        "val_idx = test_val_idx[int(Ntest_val/2):]\n",
        "\n",
        "X_ts = X[test_idx]\n",
        "y_ts = y[test_idx]\n",
        "X_vl = X[val_idx]\n",
        "y_vl = y[val_idx]\n",
        "X_tr = np.delete(X, test_val_idx)\n",
        "y_tr = np.delete(y, test_val_idx)\n",
        "\n",
        "print('The shape of X_ts is: ', X_ts.shape)\n",
        "print('The shape of y_ts is: ', y_ts.shape)\n",
        "print('The shape of X_vl is: ', X_vl.shape)\n",
        "print('The shape of y_vl is: ', y_vl.shape)\n",
        "print('The shape of X_tr is: ', X_tr.shape)\n",
        "print('The shape of y_tr is: ', y_tr.shape)\n",
        "\n",
        "# we forget about the testing data for now\n",
        "\n",
        "\n",
        "# here 1 means that the patient is healthy\n",
        "# and 0 means that the patient has cancer\n",
        "plt.scatter(X_tr,y_tr)\n",
        "plt.xlabel('Patient feature')\n",
        "plt.ylabel('Healthy or Not healthy')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oE4a8pUfsMcp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# we want to find the best values for bias and slope\n",
        "# since we do not know them, let's initialize them to be\n",
        "# random numbers from a normal distribution with zero mean\n",
        "# and a small variance (much smaller than 1)\n",
        "\n",
        "bias = # your code here\n",
        "slope = # your code here\n",
        "\n",
        "# now we can get y_hat for the training data\n",
        "# challenge: can you do this with linear algebra multiplying a vector \n",
        "# (containing the bias and slope) by a matrix containing the datapoints?\n",
        "score = # your code here\n",
        "y_hat = # your code here\n",
        "\n",
        "# let's visualize y_hat and the datapoints\n",
        "plt.scatter(X_tr,y_tr)\n",
        "plt.scatter(X_tr, y_hat, c='r')\n",
        "plt.title('Initial y_hat and datapoints')\n",
        "plt.show()\n",
        "\n",
        "# define the learning rate\n",
        "learning_rate = # your code here\n",
        "\n",
        "# now we put everything in a for loop, so that we can repeat the process\n",
        "Niters = # your code here\n",
        "\n",
        "for iter in range(Niters):\n",
        "  \n",
        "  # calculate the y_hat with the slope and bias values\n",
        "  # for the training set\n",
        "  score = # your code here\n",
        "  y_hat = # your code here\n",
        "  # and also the cost function\n",
        "  J = # your code here\n",
        "  \n",
        "  # now do the same for the validation set\n",
        "  score = # your code here\n",
        "  y_hat_vl = # your code here\n",
        "  J_vl = # your code here\n",
        "\n",
        "  # update the bias and slope terms\n",
        "  dJdbias = # your code here\n",
        "  dJdslope = # your code here\n",
        "  bias = bias - learning_rate*dJdbias\n",
        "  slope = slope - learning_rate*dJdslope  \n",
        "\n",
        "  print('At iteration No. ' + str(iter) + ' ,the cross-entropy (training) cost is: ', J)\n",
        "  print('------------------------------ the cross-entropy (validation) cost is: ', J_vl)\n",
        "\n",
        "# let's visualize y_hat and the datapoints\n",
        "# after training\n",
        "plt.scatter(X_tr,y_tr)\n",
        "plt.scatter(X_tr, y_hat, c='r')\n",
        "plt.title('Final y_hat and datapoints (training)')\n",
        "plt.show()\n",
        "plt.scatter(X_vl,y_vl)\n",
        "plt.scatter(X_vl, y_hat_vl, c='r')\n",
        "plt.title('Final y_hat and datapoints (validation)')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rIiyErEaCy7m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# now we calculate the accuracy\n",
        "\n",
        "# to do this, we need to turn the values \n",
        "# in y_hat and y_hat_val into zeros and ones\n",
        "# any value above 0.5 will become 1, and any value\n",
        "# under 0.5 will become zero. 0.5 will become zero\n",
        "# hint: use np.round\n",
        "\n",
        "y_hat_0_1 = \n",
        "y_hat_vl_0_1 = \n",
        "\n",
        "print('The training accuracy is: ', np.sum(y_tr==y_hat_0_1/Ntrain)\n",
        "print('The validation accuracy is: ', np.sum(y_vl==y_hat_vl_0_1/Nval)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-bUtW6wQ-cAx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# more things to try\n",
        "\n",
        "# create plots to visualize the training and validation loss and accuracies\n",
        "# as a function of training iterations. \n",
        "\n",
        "# if you use more than one feature, you should be able to bring the training\n",
        "# cost very close to zero. How low can you bring the validation cost?\n",
        "\n",
        "# how do you know when to stop your training? define a rule to stop your\n",
        "# training, and substitute the for loop for a while loop\n",
        "\n",
        "# once you find the absolue best model, check the test accuracy. \n",
        "# do this only once. Doing it more than one time is bad practice and makes\n",
        "# your cross-validation efforts useless."
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}