{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MNIST_NN_classification.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "wTJsCDqM7Xc6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Neural network (you do not have to do anything in this cell)\n",
        "\n",
        "# import the libraries that we will need\n",
        "from sklearn.datasets import fetch_openml  # !!! make sure you check out                                           \n",
        "import numpy as np                         # openml.org for more datasets!\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# Now we load the MNIST dataset using openml\n",
        "mnist = fetch_openml('mnist_784', cache=False)  # !!! make sure you check out\n",
        "                                                # openml.org for more datasets!\n",
        "X = mnist.data\n",
        "y = mnist.target\n",
        "\n",
        "# let's normalize X so that all values are between zero and one\n",
        "X = X/np.max(X)\n",
        "\n",
        "# let's convert y into a sparse matrix\n",
        "Ndatapoints = y.shape[0]\n",
        "Nlabels = len(np.unique(y))\n",
        "y_sparse = np.zeros([Ndatapoints, Nlabels])\n",
        "for i in range(Ndatapoints):\n",
        "  y_sparse[i,int(y[i])] = 1\n",
        "y = y_sparse\n",
        "\n",
        "print('The shape of X is: ', X.shape)\n",
        "print('The shape of y is: ', y.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZExLCWA7iCt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# now we split the data (you do not have to do anything in this cell)\n",
        "\n",
        "# because we want to use cross validation, \n",
        "# we randomly select 10% as test, 10% as validation, and 80% as training\n",
        "Ntotal = X.shape[0]\n",
        "Ntest_val = int(Ntotal/5)\n",
        "Nval = Ntest_val/2\n",
        "Ntrain = Ntotal - Ntest_val\n",
        "\n",
        "# now let's generate the indices for the test and val\n",
        "test_val_idx = np.random.choice(range(Ntotal),Ntest_val,replace=False)\n",
        "test_idx = test_val_idx[:int(Ntest_val/2)]\n",
        "val_idx = test_val_idx[int(Ntest_val/2):]\n",
        "\n",
        "X_ts = X[test_idx]\n",
        "y_ts = y[test_idx]\n",
        "X_vl = X[val_idx]\n",
        "y_vl = y[val_idx]\n",
        "X_tr = np.delete(X, test_val_idx, axis=0)\n",
        "y_tr = np.delete(y, test_val_idx, axis=0)\n",
        "\n",
        "print('The shape of X_ts is: ', X_ts.shape)\n",
        "print('The shape of y_ts is: ', y_ts.shape)\n",
        "print('The shape of X_vl is: ', X_vl.shape)\n",
        "print('The shape of y_vl is: ', y_vl.shape)\n",
        "print('The shape of X_tr is: ', X_tr.shape)\n",
        "print('The shape of y_tr is: ', y_tr.shape)\n",
        "# we forget about the testing data for now\n",
        "\n",
        "# let's visualize six examples in the training set\n",
        "Nexamples = 6\n",
        "example_idxs = np.random.choice(Ntrain,Nexamples,replace=False)\n",
        "for i, iexample in enumerate(example_idxs):\n",
        "  plt.subplot(2,3,i+1)\n",
        "  plt.imshow(np.reshape(X_tr[iexample],[28,28]),cmap='gray')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oE4a8pUfsMcp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# some variables that we will need\n",
        "Ntrain = # your code here\n",
        "Nfeatures = # your code here\n",
        "Nclasses = # your code here\n",
        "Nval = # your code here\n",
        "\n",
        "# the number of \"neurons\" in the hidden layer\n",
        "Nhidden1 = # your code here\n",
        "\n",
        "# initialize the parameters that we will train\n",
        "W_1 = # your code here\n",
        "b_1 = # your code here\n",
        "W_o = # your code here\n",
        "b_o = # your code here\n",
        "\n",
        "# now we can get y_hat for the training data\n",
        "h1_tr = # your code here\n",
        "scores = # your code here\n",
        "y_hat = # your code here\n",
        "\n",
        "# define the learning rate\n",
        "learning_rate = # your code here\n",
        "reg_str = # your code here\n",
        "\n",
        "# now we put everything in a for loop, so that we can repeat the process\n",
        "Niters = # your code here\n",
        "\n",
        "for iter in range(Niters):\n",
        "  \n",
        "  ###################################\n",
        "  ### Neural Network Forward Pass ###\n",
        "  ###################################\n",
        "  # calculate the y_hat with the slope and bias values\n",
        "  # for the training set\n",
        "  h1_tr = # your code here\n",
        "  scores = # your code here\n",
        "  y_hat = # your code here\n",
        "  # and also the cost function  \n",
        "  J = # your code here\n",
        "  \n",
        "  # now do the same for the validation set\n",
        "  h1_vl = # your code here\n",
        "  scores = # your code here\n",
        "  y_hat_vl = # your code here\n",
        "  # and also the cost function\n",
        "  J_vl = # your code here\n",
        "  \n",
        "  ####################################\n",
        "  ### Neural Network Backward Pass ###\n",
        "  #################################### \n",
        "  # update the bias and weights\n",
        "  dJdscores = # your code here\n",
        "  dJdh1 = # your code here\n",
        "  dJdh1raw = # your code here\n",
        "  dJdb_o = # your code here\n",
        "  dJdW_o = # your code here\n",
        "  dJdb_1 = # your code here\n",
        "  dJdW_1 = # your code here\n",
        "  b_o = b_o - learning_rate*dJdb_o\n",
        "  W_o = W_o - learning_rate*dJdW_o\n",
        "  b_1 = b_1 - learning_rate*dJdb_1\n",
        "  W_1 = W_1 - learning_rate*dJdW_1\n",
        "\n",
        "\n",
        "  print('At iteration No. ' + str(iter) + ' ,the cross-entropy (training) cost is: ', J)\n",
        "  print('------------------------------ the cross-entropy (validation) cost is: ', J_vl)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R1Kjf8CrM4zJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# now let's visualize some of the weights in the first layer\n",
        "for i in range(Nclasses):\n",
        "  plt.subplot(2,5,i+1)\n",
        "  plt.imshow(np.reshape(W_1[:,i],[28,28]),cmap='gray')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rIiyErEaCy7m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# now we calculate the accuracy\n",
        "\n",
        "y_hat_index = np.argmax(y_hat,axis=1)\n",
        "y_hat_vl_index = np.argmax(y_hat_vl,axis=1)\n",
        "y_tr_index = np.argmax(y_tr,axis=1)\n",
        "y_vl_index = np.argmax(y_vl,axis=1)\n",
        "\n",
        "print('The training accuracy is: ', np.sum(y_tr_index==y_hat_index)/Ntrain)\n",
        "print('The validation accuracy is: ', np.sum(y_vl_index==y_hat_vl_index)/Nval)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-bUtW6wQ-cAx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# more things to try:\n",
        "\n",
        "# try different activation functions in different layers.\n",
        "\n",
        "# implement L1 regularization\n",
        "\n",
        "# add more layers to the neural network. \n",
        "# More layers usually results in better accuracy\n",
        "\n",
        "# However, the more layers you add, the slower the neural network\n",
        "# will train. How can you speed up your network?\n",
        "\n",
        "# Get the best validation accuracy by trying out different learning rate\n",
        "# regularization strength, initial conditions for bias and Weight terms, \n",
        "# changing the statistics of the input\n",
        "\n",
        "# can you bring the training accuracy to 100%? If you do everything right\n",
        "# and you do not worry about the validation accuracy, you should be able to\n",
        "# bring the training accuracy to 100% and the training cost very close to 0\n",
        "# hint: you will have to change the size of the training set to do this\n",
        "\n",
        "# what is happening when you bring the training accuracy to 100%? \n",
        "# What happened to the validation accuracy? Can you bring both the training \n",
        "# and validation accuracy to 100%?\n",
        "\n",
        "# how do you know when to stop your training? define a rule to stop your\n",
        "# training, and substitute the for loop for a while loop\n",
        "\n",
        "# once you find the absolue best model, check the test accuracy. \n",
        "# do this only once. Doing it more than one time is bad practice and makes\n",
        "# your cross-validation efforts useless."
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}